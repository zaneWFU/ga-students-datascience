{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bilindhajer/Desktop/atl-dat2/lessons/lesson-14/code/starter-code\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def structure_file(string):\n",
    "    string_split = string.split(\"|\") # list of length 4\n",
    "    \n",
    "    first_name = string_split[0]\n",
    "    last_name  = string_split[1]\n",
    "    \n",
    "    age = int(string_split[2])\n",
    "    \n",
    "    emails = string_split[3].split(\",\")\n",
    "    email_1 = emails[0]\n",
    "    email_2 = emails[1]\n",
    "    emails = [email_1,email_2]\n",
    "    \n",
    "    final_obj = { \n",
    "        \"first_name\" : first_name,\n",
    "        \"last_name\"  : last_name,\n",
    "        \"age\" : age, \n",
    "        \"emails\" : emails\n",
    "    }\n",
    "    \n",
    "    return final_obj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_1 = \"Bilind|Hajer|27|bilind07@gmail.com,bhajer3@gatech.edu\"\n",
    "sentence_2 = \"John|Smith|24|JS12@gmail.com,jSmith3@gatech.edu\"\n",
    "sentence_3 = \"Sarah|Adams|22|sarah22@gmail.com,sAdams3@gatech.edu\"\n",
    "sentence_4 = \"Rob|Adams|24|rob24@gmail.com,rAdams3@gatech.edu\"\n",
    "textfile = [sentence_1,sentence_2,sentence_3,sentence_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textfile_structured = map(structure_file,textfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_name': 'Bilind', 'last_name': 'Hajer', 'age': 27, 'emails': ['bilind07@gmail.com', 'bhajer3@gatech.edu']}\n",
      "{'first_name': 'John', 'last_name': 'Smith', 'age': 24, 'emails': ['JS12@gmail.com', 'jSmith3@gatech.edu']}\n",
      "{'first_name': 'Sarah', 'last_name': 'Adams', 'age': 22, 'emails': ['sarah22@gmail.com', 'sAdams3@gatech.edu']}\n",
      "{'first_name': 'Rob', 'last_name': 'Adams', 'age': 24, 'emails': ['rob24@gmail.com', 'rAdams3@gatech.edu']}\n"
     ]
    }
   ],
   "source": [
    "for obj in textfile_structured:\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# give me only student's bilind's record\n",
    "student = filter(\n",
    "    lambda obj: obj[\"first_name\"].lower() == \"bilind\",\n",
    "    textfile_structured\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'first_name': 'Bilind', 'last_name': 'Hajer', 'age': 27, 'emails': ['bilind07@gmail.com', 'bhajer3@gatech.edu']}]\n"
     ]
    }
   ],
   "source": [
    "print(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all students atleast 24 years old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_sentence = \"Hi, my name is Bilind, I like to run, jog, and eat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_to_unicode = unicode(new_sentence, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bilind Hajer 27 bilind07@gmail.com,bhajer3@gatech.edu\n"
     ]
    }
   ],
   "source": [
    "print(setenence_to_unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp_toolkit = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed = nlp_toolkit(sentence_to_unicode) # parse or tokenize the unicode object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entities = parsed.ents # grade entities of tokenized unicode object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mentions_name(entities, name):\n",
    "    for entity in entities:\n",
    "        if entity.text == name:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_an_action(token):\n",
    "    if(token.pos == spacy.parts_of_speech.VERB):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_actions(parsed):\n",
    "    actions = filter(is_an_action,parsed)\n",
    "    actions = map(lambda token: token.lemma_ , actions)\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print mentions_name(entities,\"Bilind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'be', u'like', u'run', u'eat']\n"
     ]
    }
   ],
   "source": [
    "print get_actions(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function that can take a take a sentence parsed by `spacy` and \n",
    "# identify if it mentions a company named 'Google'. \n",
    "# Remember, `spacy` can find entities and code them `ORG` if they are a company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function that can take a sentence parsed by `spacy` \n",
    "# and return the verbs of the sentence (preferably lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function that identifies countries - HINT: the entity label for \n",
    "# countries is GPE (or GeoPolitical Entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "if __name__ == '__main__':\n",
    "    # Loading the tweet data\n",
    "    tweets = [unicode(tweet, errors='ignore') for tweet in open('../../assets/dataset/captured-tweets.txt', 'r')]\n",
    "\n",
    "    # Setting up spacy\n",
    "    nlp_toolkit = English()\n",
    "\n",
    "    # For each tweet, parse it using `spacy` and print out if the tweet \n",
    "    # has 'release' or 'announce' as a verb.\n",
    "    for tweet in tweets:\n",
    "        parsed = nlp_toolkit(tweet)\n",
    "\n",
    "        if mentions_company(parsed, 'Google'):\n",
    "            actions = get_actions(parsed)\n",
    "            if 'release' in actions or 'announce' in actions:\n",
    "                print(tweet)\n",
    "\n",
    "    # Solution to 1f\n",
    "    # Re-run (d) find country tweets that discuss 'Iran' announcing or releasing.\n",
    "    for tweet in tweets:\n",
    "        parsed = nlp_toolkit(tweet)\n",
    "\n",
    "        if mentions_country(parsed, 'Iran'):\n",
    "            actions = get_actions(parsed)\n",
    "            if 'release' in actions or 'announce' in actions:\n",
    "                print(tweet)\n",
    "\n",
    "    # First take the collection of tweets and tokenize them using `spacy`\n",
    "\n",
    "    # I decided to lemmatized the verbs for easier searching and keep symbols\n",
    "    # and punctuations\n",
    "\n",
    "    text_split = [[x.text if x.pos != spacy.parts_of_speech.VERB else x.lemma_ \n",
    "                    for x in nlp_toolkit(t)] for t in tweets]\n",
    "\n",
    "    model = Word2Vec(text_split, size=100, window=4, min_count=5, workers=4)\n",
    "\n",
    "    model.most_similar(positive=['Syria'])\n",
    "\n",
    "    # Filter tweets down to those that mention 'Iran' or similar entities and \n",
    "    # 'war' or similar entities\n",
    "\n",
    "    # a: Using spacy\n",
    "    for tweet in tweets:\n",
    "        parsed = nlp_toolkit(tweet)\n",
    "        if mentions_country(parsed, 'Iran') or mentions_country(parsed, 'Iraq'): # ... you could add more\n",
    "            if 'attack' in get_actions(parsed):\n",
    "                print(tweet)\n",
    "\n",
    "    # b: using similarity scores\n",
    "    for tweet in tweets:\n",
    "        parsed = nlp_toolkit(tweet)\n",
    "\n",
    "        similarity_to_iran = max([model.similarity('Iran', tok.text) for tok in parsed if tok.text in model.vocab], 0)\n",
    "        similarity_to_war = max([model.similarity('war', tok.text) for tok in parsed if tok.text in model.vocab], 0)\n",
    "        if similarity_to_iran > 0.75 and similarity_to_war > 0.75:\n",
    "            print(similarity_to_iran, similarity_to_war, tweet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
