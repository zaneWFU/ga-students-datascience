{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is WINDOWS\n",
      " Volume Serial Number is AA1C-00BD\n",
      "\n",
      " Directory of C:\\Users\\Zane_Harris\\Documents\\GitHub\\ga-students-datascience\\lessons\\lesson-14\\code\\starter-code\n",
      "\n",
      "04/14/2016  08:30 PM    <DIR>          .\n",
      "04/14/2016  08:30 PM    <DIR>          ..\n",
      "04/14/2016  06:42 PM    <DIR>          .ipynb_checkpoints\n",
      "02/25/2016  07:08 PM               549 capture-tweets.py\n",
      "04/05/2016  07:21 PM             4,151 helper.ipynb\n",
      "02/25/2016  07:08 PM               864 starter-code-14.py\n",
      "04/05/2016  07:21 PM             6,086 starter-code-final.ipynb\n",
      "04/06/2016  11:36 AM            14,804 starter-code-final_retry.ipynb\n",
      "04/14/2016  08:30 PM            21,511 structuring_nlp_helper.ipynb\n",
      "02/25/2016  07:08 PM               979 twitter.py\n",
      "04/05/2016  11:55 PM             9,640 Untitled.ipynb\n",
      "               8 File(s)         58,584 bytes\n",
      "               3 Dir(s)  130,490,863,616 bytes free\n",
      "Requirement already satisfied (use --upgrade to upgrade): datetime in c:\\users\\zane_harris\\appdata\\local\\continuum\\anaconda2\\lib\\site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): zope.interface in c:\\users\\zane_harris\\appdata\\local\\continuum\\anaconda2\\lib\\site-packages (from datetime)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pytz in c:\\users\\zane_harris\\appdata\\local\\continuum\\anaconda2\\lib\\site-packages (from datetime)\n",
      "Requirement already satisfied (use --upgrade to upgrade): setuptools in c:\\users\\zane_harris\\appdata\\local\\continuum\\anaconda2\\lib\\site-packages\\setuptools-19.6.2-py2.7.egg (from zope.interface->datetime)\n"
     ]
    }
   ],
   "source": [
    "!dir\n",
    "!pip install datetime\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def structure_file(string):\n",
    "    string_split = string.split(\"|\") # list of length 4\n",
    "    \n",
    "    first_name = string_split[0]\n",
    "    last_name  = string_split[1]\n",
    "    \n",
    "    age = int(string_split[2])\n",
    "    \n",
    "    emails = string_split[3].split(\",\")\n",
    "    email_1 = emails[0]\n",
    "    email_2 = emails[1]\n",
    "    emails = [email_1,email_2]\n",
    "    \n",
    "    dob = get_dob_from_string(string_split[4])\n",
    "    \n",
    "    final_obj = { \n",
    "        \"first_name\" : first_name,\n",
    "        \"last_name\"  : last_name,\n",
    "        \"age\" : age, \n",
    "        \"emails\" : emails,\n",
    "        \"dob\" : dob\n",
    "    }\n",
    "    \n",
    "    return final_obj\n",
    "\n",
    "def get_dob_from_string(dob):\n",
    "    return datetime.datetime.strptime(dob, \"%Y-%m-%d\").date()\n",
    "def get_month_from_string(dob):\n",
    "    return datetime.datetime.strptime(dob, \"%Y-%m-%d\").month\n",
    "def bin_month_on_season(month):\n",
    "    if month in [1,2,3]:\n",
    "        return 1\n",
    "    elif month in [4,5,6]:\n",
    "        return 2\n",
    "    elif month in [7,8,9]:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_1 = \"Bilind|Hajer|27|bilind07@gmail.com,bhajer3@gatech.edu|1989-05-28\"\n",
    "sentence_2 = \"John|Smith|24|JS12@gmail.com,jSmith3@gatech.edu|1990-04-20\"\n",
    "sentence_3 = \"Sarah|Adams|22|sarah22@gmail.com,sAdams3@gatech.edu|1991-03-30\"\n",
    "sentence_4 = \"Rob|Adams|24|rob24@gmail.com,rAdams3@gatech.edu|1990-02-15\"\n",
    "textfile = [sentence_1,sentence_2,sentence_3,sentence_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textfile_structured = map(structure_file,textfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dob': datetime.date(1989, 5, 28), 'first_name': 'Bilind', 'last_name': 'Hajer', 'age': 27, 'emails': ['bilind07@gmail.com', 'bhajer3@gatech.edu']}\n",
      "{'dob': datetime.date(1990, 4, 20), 'first_name': 'John', 'last_name': 'Smith', 'age': 24, 'emails': ['JS12@gmail.com', 'jSmith3@gatech.edu']}\n",
      "{'dob': datetime.date(1991, 3, 30), 'first_name': 'Sarah', 'last_name': 'Adams', 'age': 22, 'emails': ['sarah22@gmail.com', 'sAdams3@gatech.edu']}\n",
      "{'dob': datetime.date(1990, 2, 15), 'first_name': 'Rob', 'last_name': 'Adams', 'age': 24, 'emails': ['rob24@gmail.com', 'rAdams3@gatech.edu']}\n"
     ]
    }
   ],
   "source": [
    "for obj in textfile_structured:\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# give me only student's bilind's record\n",
    "student = filter(\n",
    "    lambda obj: obj[\"first_name\"].lower() == \"bilind\",\n",
    "    textfile_structured\n",
    ")\n",
    "\n",
    "young_students = filter(lambda obj: obj[\"dob\"] >= datetime.date(1991, 1, 1), textfile_structured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bilind\n",
      "Bilind\n"
     ]
    }
   ],
   "source": [
    "print(student[0][\"first_name\"])\n",
    "\n",
    "for record in student:\n",
    "    print record[\"first_name\"]\n",
    "    \n",
    "student_24_over = filter(lambda obj: obj[\"age\"] >= 24, textfile_structured)\n",
    "\n",
    "\"adams\" in \"rAdams3@gatech.edu\".lower()\n",
    "\n",
    "def check_name_adams(obj):\n",
    "    return \"adams\" in obj[\"emails\"][0].lower() or \"adams\" in obj[\"emails\"][1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dob': datetime.date(1989, 5, 28), 'first_name': 'Bilind', 'last_name': 'Hajer', 'age': 27, 'emails': ['bilind07@gmail.com', 'bhajer3@gatech.edu']}]\n"
     ]
    }
   ],
   "source": [
    "print(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all students atleast 24 years old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_sentence = \"Hi, my name is Bilind, I like to run, jog, and eat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_to_unicode = unicode(new_sentence, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, my name is Bilind, I like to run, jog, and eat\n"
     ]
    }
   ],
   "source": [
    "print(sentence_to_unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp_toolkit = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed = nlp_toolkit(sentence_to_unicode) # parse or tokenize the unicode object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entities = parsed.ents # grade entities of tokenized unicode object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mentions_name(entities, name):\n",
    "    for entity in entities:\n",
    "        if entity.text == name:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_an_action(token):\n",
    "    if(token.pos == spacy.parts_of_speech.VERB):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_actions(parsed):\n",
    "    actions = filter(is_an_action,parsed)\n",
    "    actions = map(lambda token: token.lemma_ , actions)\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print mentions_name(entities,\"Bilind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'be', u'like', u'run', u'eat']\n"
     ]
    }
   ],
   "source": [
    "print get_actions(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function that can take a take a sentence parsed by `spacy` and \n",
    "# identify if it mentions a company named 'Google'. \n",
    "# Remember, `spacy` can find entities and code them `ORG` if they are a company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function that can take a sentence parsed by `spacy` \n",
    "# and return the verbs of the sentence (preferably lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function that identifies countries - HINT: the entity label for \n",
    "# countries is GPE (or GeoPolitical Entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name interfaces",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-b2df8fb814f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Loading the tweet data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../assets/dataset/captured-tweets.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Zane_Harris\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\gensim\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \"\"\"\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Zane_Harris\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\gensim\\models\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# bring model classes directly into package namespace, to save some typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mhdpmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHdpModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mldamodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLdaModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlsimodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLsiModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Zane_Harris\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\gensim\\models\\hdpmodel.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name interfaces"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "if __name__ == '__main__':\n",
    "    # Loading the tweet data\n",
    "    tweets = [unicode(tweet, errors='ignore') for tweet in open('../../assets/dataset/captured-tweets.txt', 'r')]\n",
    "\n",
    "    # Setting up spacy\n",
    "    nlp_toolkit = English()\n",
    "\n",
    "    # For each tweet, parse it using `spacy` and print out if the tweet \n",
    "    # has 'release' or 'announce' as a verb.\n",
    "    for tweet in tweets:\n",
    "        parsed = nlp_toolkit(tweet)\n",
    "\n",
    "        if mentions_company(parsed, 'Google'):\n",
    "            actions = get_actions(parsed)\n",
    "            if 'release' in actions or 'announce' in actions:\n",
    "                print(tweet)\n",
    "\n",
    "    # Solution to 1f\n",
    "    # Re-run (d) find country tweets that discuss 'Iran' announcing or releasing.\n",
    "    for tweet in tweets:\n",
    "        parsed = nlp_toolkit(tweet)\n",
    "\n",
    "        if mentions_country(parsed, 'Iran'):\n",
    "            actions = get_actions(parsed)\n",
    "            if 'release' in actions or 'announce' in actions:\n",
    "                print(tweet)\n",
    "\n",
    "    # First take the collection of tweets and tokenize them using `spacy`\n",
    "\n",
    "    # I decided to lemmatized the verbs for easier searching and keep symbols\n",
    "    # and punctuations\n",
    "\n",
    "    text_split = [[x.text if x.pos != spacy.parts_of_speech.VERB else x.lemma_ \n",
    "                    for x in nlp_toolkit(t)] for t in tweets]\n",
    "\n",
    "    model = Word2Vec(text_split, size=100, window=4, min_count=5, workers=4)\n",
    "\n",
    "    model.most_similar(positive=['Syria'])\n",
    "\n",
    "    # Filter tweets down to those that mention 'Iran' or similar entities and \n",
    "    # 'war' or similar entities\n",
    "\n",
    "    # a: Using spacy\n",
    "    for tweet in tweets:\n",
    "        parsed = nlp_toolkit(tweet)\n",
    "        if mentions_country(parsed, 'Iran') or mentions_country(parsed, 'Iraq'): # ... you could add more\n",
    "            if 'attack' in get_actions(parsed):\n",
    "                print(tweet)\n",
    "\n",
    "    # b: using similarity scores\n",
    "    for tweet in tweets:\n",
    "        parsed = nlp_toolkit(tweet)\n",
    "\n",
    "        similarity_to_iran = max([model.similarity('Iran', tok.text) for tok in parsed if tok.text in model.vocab], 0)\n",
    "        similarity_to_war = max([model.similarity('war', tok.text) for tok in parsed if tok.text in model.vocab], 0)\n",
    "        if similarity_to_iran > 0.75 and similarity_to_war > 0.75:\n",
    "            print(similarity_to_iran, similarity_to_war, tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
